{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Assignment: Data and Model Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering (Audio Data Augmentation)\n",
    "\n",
    "* Add noise to the audio data\n",
    "* Add random pitch shift to the audio data\n",
    "* Add random time shift to the audio data\n",
    "* specAugment augmentation technique on raw audio file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of hyperparameters that can be tuned for when working with audio data for keyword spotting/speech recognition tasks:\n",
    "\n",
    "1. Acoustic Model:\n",
    "\n",
    "* Number of hidden states in the recurrent neural network (RNN)\n",
    "* Type of RNN (e.g. GRU, LSTM)\n",
    "* Number of layers in the RNN\n",
    "* Number of epochs used for training\n",
    "* Batch size\n",
    "* Learning rate\n",
    "* Dropout rate\n",
    "* Number of filter banks in the feature extraction layer\n",
    "\n",
    "2. Language Model:\n",
    "\n",
    "* N-gram order (e.g. unigram, bigram, trigram)\n",
    "* Smoothing method (e.g. Laplace smoothing, Kneser-Ney smoothing)\n",
    "* Interpolation weight for combining different n-gram models\n",
    "* Context window size\n",
    "\n",
    "3. Beam Search Decoding:\n",
    "\n",
    "* Beam size\n",
    "* Pruning threshold\n",
    "* Maximum number of search steps\n",
    "\n",
    "4. Feature Extraction:\n",
    "\n",
    "* Type of filter bank (e.g. Mel-frequency cepstral coefficients (MFCCs), spectrogram)\n",
    "* Window size for the short-time Fourier transform (STFT)\n",
    "* Window type for the STFT (e.g. Hanning, Hamming)\n",
    "* Frame rate\n",
    "* Number of filter banks\n",
    "* Frequency range\n",
    "\n",
    "5. Data Augmentation:\n",
    "\n",
    "* Amount of noise added to the speech signal\n",
    "* Number of synthetic speech samples generated\n",
    "* Method for generating synthetic speech samples (e.g. text-to-speech synthesis)\n",
    "\n",
    "6. Regularization:\n",
    "\n",
    "* L1/L2 regularization weight\n",
    "* Dropout rate\n",
    "\n",
    "7. Fine-Tuning (Fine-tuning is a technique where a pre-trained model is further trained on a smaller dataset specific to the task at hand. This can be useful for key-word spotting as it allows the model to learn the specific characteristics of the target keywords):\n",
    "\n",
    "* Learning rate\n",
    "* Number of epochs used for fine-tuning\n",
    "* Batch size\n",
    "* Dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to leverage AutoML\n",
    "\n",
    "* Use AutoML to benchmark your model\n",
    "* Use AutoML to tune hyperparameters\n",
    "\n",
    "\n",
    "        We have not yet figured out how to implement the above in our project. We will be working on this in the next few weeks.\n",
    "        We will be mainly use AutoML to mkae our model smaller and ready for deployment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document performance, interpretation, and learnings in markdown\n",
    "\n",
    "From the [Few-Shot Keyword Spotting in Any Language, Interspeech 2021](https://www.isca-speech.org/archive/pdfs/interspeech_2021/mazumder21_interspeech.pdf): Copyright © 2021 ISCA\n",
    "\n",
    "\"5-Shot KWS Classification Accuracy. ROC curves for 5-shot KWS models with 20 randomly selected keywords per language. For each language, the mean is drawn as ab olded curve over the shaded standard deviation(all keywords are shown as a hairline trace).\n",
    "\n",
    "(a) 5-shot KWS models using an embedding representation trained per language for six languages.\n",
    "\n",
    "[Average F1@0.8 = 0.58]\n",
    "\n",
    "(b) 5shot models using a multilingual embedding trained on nine languages — accuracy improves relative to (a).\n",
    "\n",
    "[Avg. F1@0.8 = 0.75]\n",
    "\n",
    "(c) 5-shot models using the same multilingual embedding from (b) for random keywords in 13 languages which are out-of-embedding (i.e., which the feature extractor has never encountered), showing that our embedding generalizes to new languages.\n",
    "\n",
    "[Avg. F1@0.8 = 0.65]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        We have yet to iteratively improve our model performance, as we have only been able to perform model comparisons between existing works. Previously we were using (1) zero-shot learning with wav2vec2-asr model and later (2) fine-tuned model of wav2vec2 for keyword spotting, but we found them having very poor performance for unseen keywords.  \n",
    "        We will be working on model improvement on the currently selected model , Harvard multilingual few shot keyword spotting , in the following weeks.\n",
    "        We also investigated possible ways for real-time keyword spotting app development, and identified possible challenges. (e.g. model deployment on iOS needs Swift programming). We may first deploy the model in a wab-base fashion; and if time allows on an app.\n",
    "        \n",
    "        Since our model uses a large volume of data, we spend a lot of time learning how to use S3 and access data from SageMaker via S3.\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report and Observation:\n",
    "\n",
    "the accuracy of a few-shot model can be quite sensitive to the particular samples chosen. A different set of five training samples will likely vary by a few percentage points from this model. The subset of unknown words and background noise also impact accuracy. Furthermore, some of the extracted test samples may be truncated (due to incorrect word boundary estimates) or occasionally anomalous due to issues with the originating crowdsourced data so a more accurate estimate of the test performance can only be made after manually listening to all test samples and discarding any malformed samples.\n",
    "\n",
    "### Next steps:\n",
    "\n",
    " - Try custom key word spotting like \"hey computer\"\n",
    " - Having an interface for few shot leanring (training), evaluation, and inference for visualization interactively on continious audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
